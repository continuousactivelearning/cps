{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###Installing Libraries"
      ],
      "metadata": {
        "id": "VRyHlC7Mt2n5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mknLR31fsN-M",
        "outputId": "772407a9-0c94-43f9-cfc0-6fda4e7e77ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Collecting rouge-score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Collecting bert-score\n",
            "  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.33.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m77.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: rouge-score\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=3521a322d649bbebb622b2ee1d3ec26d7fdb4603097070240062385dc45ee34e\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
            "Successfully built rouge-score\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, rouge-score, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bert-score\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed bert-score-0.3.13 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 rouge-score-0.1.2\n"
          ]
        }
      ],
      "source": [
        "pip install pandas numpy nltk rouge-score sentence-transformers transformers torch bert-score matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk;\n",
        "nltk.download('all')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F648GKAft1SD",
        "outputId": "b6420195-9d36-43da-93b7-207624e8dc58"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_rus.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package english_wordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/english_wordnet.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/maxent_treebank_pos_tagger_tab.zip.\n",
            "[nltk_data]    | Downloading package mock_corpus to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mock_corpus.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package tagsets_json to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets_json.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Creating Taxonomy List"
      ],
      "metadata": {
        "id": "pwsW3hdSvCnC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import os\n",
        "import networkx as nx\n",
        "\n",
        "# Load dataset to compute skill frequencies\n",
        "df_freq = pd.read_csv('/content/drive/MyDrive/EduVLM/gsm8k_wrong_answers_with_missing_prerequisites_enhanced.csv')\n",
        "\n",
        "# Skill taxonomy and synonyms\n",
        "skill_taxonomy = {\n",
        "    'addition': [], 'subtraction': ['addition'], 'multiplication': ['addition'],\n",
        "    'division': ['multiplication', 'addition'], 'fraction': ['division', 'multiplication', 'addition'],\n",
        "    'percentage': ['multiplication', 'division', 'fraction'], 'percentages': ['multiplication', 'division', 'fraction'],\n",
        "    'algebraic thinking': ['multiplication', 'addition', 'subtraction'], 'ratios': ['fraction', 'division', 'multiplication'],\n",
        "    'geometry': ['measurement', 'addition', 'angles'], 'measurement': ['addition', 'number sense'],\n",
        "    'algebra': ['algebraic thinking', 'multiplication', 'subtraction'], 'proportion': ['fraction', 'division', 'ratios'],\n",
        "    'rate': ['fraction', 'division', 'ratios'], 'area': ['multiplication', 'measurement'],\n",
        "    'volume': ['multiplication', 'measurement', 'area'], 'proportional': ['ratios', 'fraction', 'proportion'],\n",
        "    'angles': ['geometry', 'measurement'], 'number sense': ['addition', 'counting'], 'counting': [], 'unknown': []\n",
        "}\n",
        "\n",
        "math_synonyms = {\n",
        "    'addition': ['add', 'sum', 'plus', 'total', 'combine', 'addition operation', 'summation'],\n",
        "    'subtraction': ['subtract', 'minus', 'difference', 'take away', 'reduce', 'subtraction operation', 'deduct'],\n",
        "    'multiplication': ['multiply', 'times', 'product', 'repeated addition', 'multiplication operation', 'scale'],\n",
        "    'division': ['divide', 'quotient', 'share', 'split', 'division operation', 'partitive'],\n",
        "    'fraction': ['ratio', 'proportion', 'frac', 'part', 'division', 'fractional', 'proper fraction'],\n",
        "    'percentage': ['percent', 'rate', 'pct', 'percentages', '%', 'percentile', 'per cent'],\n",
        "    'percentages': ['percent', 'rate', 'pct', 'percentage', '%', 'percentile', 'per cent'],\n",
        "    'algebraic thinking': ['algebra', 'equations', 'variables', 'unknowns', 'algebraic', 'linear equations'],\n",
        "    'ratios': ['proportion', 'rate', 'ratio', 'fraction', 'scale', 'proportional', 'relative'],\n",
        "    'geometry': ['shapes', 'measurement', 'area', 'volume', 'spatial', 'geometric', 'angles'],\n",
        "    'measurement': ['measure', 'length', 'area', 'volume', 'units', 'dimension', 'metric'],\n",
        "    'algebra': ['equations', 'variables', 'algebraic thinking', 'algebraic expressions', 'polynomials'],\n",
        "    'proportion': ['ratio', 'fraction', 'rate', 'proportional', 'scaling'],\n",
        "    'rate': ['proportion', 'ratio', 'speed', 'frequency', 'rate of change'],\n",
        "    'area': ['surface area', 'square units', 'multiplication', 'measurement'],\n",
        "    'volume': ['cubic units', 'multiplication', 'measurement', 'capacity'],\n",
        "    'proportional': ['ratios', 'fraction', 'proportion', 'scaling'],\n",
        "    'angles': ['geometry', 'measurement'],\n",
        "    'number sense': ['counting', 'numeracy', 'number operations'],\n",
        "    'computational error': ['computational errors', 'calculation mistakes', 'wrong operations']\n",
        "}\n",
        "\n",
        "# Normalize skill function\n",
        "def normalize_skill(skill, synonyms):\n",
        "    skill = str(skill).lower().strip()\n",
        "    for canonical, syn_list in synonyms.items():\n",
        "        if skill == canonical or skill in [s.lower() for s in syn_list]:\n",
        "            return canonical\n",
        "    if any(x in skill for x in ['$', '1/', 'computational', 'incorrect', 'mistake', 'error']):\n",
        "        return 'computational error'\n",
        "    return 'unknown'\n",
        "\n",
        "# Extract skills for IC\n",
        "def extract_skills(row):\n",
        "    skills = str(row.get('missing_prerequisites', '')).lower().split()\n",
        "    return [skill for skill in skills if skill in skill_taxonomy]\n",
        "\n",
        "# Compute Information Content (IC)\n",
        "skill_counts = Counter()\n",
        "for index, row in df_freq.iterrows():\n",
        "    skill_counts.update(extract_skills(row))\n",
        "\n",
        "total_skills = sum(skill_counts.values())\n",
        "if total_skills > 0:\n",
        "    ic = {skill: -np.log(skill_counts[skill] / total_skills) if skill_counts[skill] > 0 else 0 for skill in skill_taxonomy}\n",
        "else:\n",
        "    ic = {skill: 0 for skill in skill_taxonomy}\n",
        "\n",
        "# Build taxonomic graph\n",
        "G = nx.DiGraph()\n",
        "for skill, prerequisites in skill_taxonomy.items():\n",
        "    G.add_node(skill, ic=ic[skill])\n",
        "    for prereq in prerequisites:\n",
        "        G.add_edge(prereq, skill)\n",
        "\n",
        "# Function to find Least Common Ancestor (LCA)\n",
        "def find_lca(graph, skill1, skill2):\n",
        "    if skill1 not in graph or skill2 not in graph:\n",
        "        return None\n",
        "    ancestors1 = nx.ancestors(graph, skill1) | {skill1}\n",
        "    ancestors2 = nx.ancestors(graph, skill2) | {skill2}\n",
        "    common_ancestors = ancestors1 & ancestors2\n",
        "    if not common_ancestors:\n",
        "        return None\n",
        "    return max(common_ancestors, key=lambda x: graph.nodes[x]['ic'])\n",
        "\n",
        "# File paths\n",
        "model_results = [\n",
        "    '/content/drive/MyDrive/EduVLM/internvl3_8b_results.csv',\n",
        "    '/content/drive/MyDrive/EduVLM/gemma3_12b_results_v2.csv',\n",
        "    '/content/drive/MyDrive/EduVLM/qwen_2.5_vl_7b_results_v2.csv',\n",
        "    '/content/drive/MyDrive/EduVLM/smolvlm2_500m_results_v2.csv'\n",
        "]"
      ],
      "metadata": {
        "id": "ZwlXoGNJuCmA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Pass@K Metrics"
      ],
      "metadata": {
        "id": "71o76ZAWvQDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pass@1\n",
        "def compute_pass_at_k(df, k=1):\n",
        "    if k == 1:\n",
        "        if 'is_correct' not in df.columns:\n",
        "            raise ValueError(\"Column 'is_correct' not found\")\n",
        "        if not df['is_correct'].isin([0, 1, 0.0, 1.0]).all():\n",
        "            print(f\"Warning: Non-binary values in 'is_correct' for {file_path}: {df['is_correct'].unique()}\")\n",
        "        return df['is_correct'].mean() * 100  # Output as percentage\n",
        "    else:\n",
        "        raise ValueError(\"Only Pass@1 is supported in this version.\")\n",
        "\n",
        "# Evaluate Pass@1 for results\n",
        "results_pass_at_1 = []\n",
        "for file_path in model_results:\n",
        "    try:\n",
        "        if os.path.exists(file_path):\n",
        "            df = pd.read_csv(file_path)\n",
        "            model_name = os.path.basename(file_path)\n",
        "            pass_at_1 = compute_pass_at_k(df, k=1)\n",
        "            results_pass_at_1.append({'Model': model_name, 'Pass@1 (%)': pass_at_1})\n",
        "            print(f\"Processed {model_name}: Pass@1 = {pass_at_1:.1f}%\")\n",
        "        else:\n",
        "            print(f\"File not found: {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {file_path}: {str(e)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVZdkdXYvJlc",
        "outputId": "09dd76e4-2bdd-4a6b-dd28-9b11cefe2a0f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed internvl3_8b_results.csv: Pass@1 = 19.0%\n",
            "Processed gemma3_12b_results_v2.csv: Pass@1 = 35.5%\n",
            "Processed qwen_2.5_vl_7b_results_v2.csv: Pass@1 = 38.5%\n",
            "Processed smolvlm2_500m_results_v2.csv: Pass@1 = 13.5%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###METEOR Score Metric"
      ],
      "metadata": {
        "id": "TB_bvO55vhBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.meteor_score import meteor_score\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Normalize skills\n",
        "def normalize_skill(skill, synonyms):\n",
        "    skill = str(skill).lower().strip()\n",
        "    for canonical, syn_list in synonyms.items():\n",
        "        if skill == canonical or skill in [s.lower() for s in syn_list]:\n",
        "            return canonical\n",
        "    return 'unknown'\n",
        "\n",
        "# Compute METEOR score for a single prediction\n",
        "def compute_meteor_score(ground_truth, predicted, synonyms):\n",
        "    try:\n",
        "        gt = normalize_skill(ground_truth, synonyms)\n",
        "        pred = normalize_skill(predicted, synonyms)\n",
        "        gt_tokens = word_tokenize(gt)\n",
        "        pred_tokens = word_tokenize(pred)\n",
        "        references = [gt_tokens] + [word_tokenize(syn) for syn in synonyms.get(gt, []) if syn]\n",
        "        if not references or not pred_tokens:\n",
        "            return 0.0\n",
        "        score = float(meteor_score(references, pred_tokens))  # Explicit float conversion\n",
        "        return score\n",
        "    except Exception as e:\n",
        "        print(f\"METEOR error for gt='{ground_truth}', pred='{predicted}': {e}\")\n",
        "        return 0.0\n",
        "\n",
        "# Evaluation function for METEOR Score\n",
        "def evaluate_meteor_score(df, model_name):\n",
        "    try:\n",
        "        if 'ground_truth' not in df.columns or 'predicted' not in df.columns:\n",
        "            raise ValueError(\"DataFrame must contain 'ground_truth' and 'predicted' columns\")\n",
        "\n",
        "        df = df.dropna(subset=['ground_truth', 'predicted'])  # Drop rows with missing values\n",
        "        meteor_scores = []\n",
        "\n",
        "        for _, row in df.iterrows():\n",
        "            ground_truth = str(row['ground_truth']).lower()\n",
        "            predicted = str(row['predicted']).lower()\n",
        "            meteor = compute_meteor_score(ground_truth, predicted, math_synonyms)\n",
        "            meteor_scores.append(meteor)\n",
        "\n",
        "        avg_meteor = np.mean(meteor_scores) if meteor_scores else 0.0\n",
        "        return {'Model': model_name, 'Average METEOR Score': avg_meteor}\n",
        "    except Exception as e:\n",
        "        print(f\"Error computing METEOR score for {model_name}: {e}\")\n",
        "        return {'Model': model_name, 'Average METEOR Score': np.nan}\n",
        "\n",
        "\n",
        "# Evaluate METEOR Score for all models\n",
        "results_meteor_score = []\n",
        "for file_path in model_results:\n",
        "    if os.path.exists(file_path):\n",
        "        df = pd.read_csv(file_path)\n",
        "        model_name = os.path.basename(file_path)\n",
        "        print(f\"\\nEvaluation for {model_name}:\")\n",
        "        metrics = evaluate_meteor_score(df, model_name)\n",
        "        results_meteor_score.append(metrics)\n",
        "        for metric, value in metrics.items():\n",
        "            if isinstance(value, (int, float)):  # Apply .3f only to numbers\n",
        "                print(f\"{metric}: {value:.3f}\")\n",
        "            else:  # Print strings as-is\n",
        "                print(f\"{metric}: {value}\")\n",
        "    else:\n",
        "        print(f\"\\nFile not found: {file_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvbawqXgvmxS",
        "outputId": "2cd3f515-975e-4f50-d376-ab43222e3628"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation for internvl3_8b_results.csv:\n",
            "Model: internvl3_8b_results.csv\n",
            "Average METEOR Score: 0.345\n",
            "\n",
            "Evaluation for gemma3_12b_results_v2.csv:\n",
            "Model: gemma3_12b_results_v2.csv\n",
            "Average METEOR Score: 0.255\n",
            "\n",
            "Evaluation for qwen_2.5_vl_7b_results_v2.csv:\n",
            "Model: qwen_2.5_vl_7b_results_v2.csv\n",
            "Average METEOR Score: 0.260\n",
            "\n",
            "Evaluation for smolvlm2_500m_results_v2.csv:\n",
            "Model: smolvlm2_500m_results_v2.csv\n",
            "Average METEOR Score: 0.227\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###BERT Score Metric"
      ],
      "metadata": {
        "id": "oCeLdwIFvXCb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import numpy as np\n",
        "# BERT Score\n",
        "def compute_bert_score(df):\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "        model = AutoModel.from_pretrained('bert-base-uncased')\n",
        "        model.eval()\n",
        "\n",
        "        def get_bert_embedding(text):\n",
        "            inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**inputs)\n",
        "            return outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n",
        "\n",
        "        ground_truth = df['ground_truth'].apply(str)\n",
        "        predicted = df['predicted'].apply(str)\n",
        "        scores = []\n",
        "        for gt, pred in zip(ground_truth, predicted):\n",
        "            gt_emb = get_bert_embedding(gt)\n",
        "            pred_emb = get_bert_embedding(pred)\n",
        "            cosine_sim = np.dot(gt_emb, pred_emb) / (np.linalg.norm(gt_emb) * np.linalg.norm(pred_emb) + 1e-8)\n",
        "            scores.append(cosine_sim)\n",
        "        return np.mean(scores)\n",
        "    except Exception as e:\n",
        "        print(f\"Error computing BERT score: {str(e)}\")\n",
        "        return np.nan\n",
        "\n",
        "# Evaluate BERT Score for results\n",
        "results_bert_score = []\n",
        "for file_path in model_results:\n",
        "    try:\n",
        "        if os.path.exists(file_path):\n",
        "            df = pd.read_csv(file_path)\n",
        "            model_name = os.path.basename(file_path)\n",
        "            bert_score = compute_bert_score(df)\n",
        "            results_bert_score.append({'Model': model_name, 'BERT Score': bert_score})\n",
        "            print(f\"Processed {model_name}: BERT Score = {bert_score:.3f}\")\n",
        "        else:\n",
        "            print(f\"File not found: {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {file_path}: {str(e)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IS4stlhuvV3I",
        "outputId": "67d1ddc7-9f5e-43ff-e38e-5c56da2dbe61"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed internvl3_8b_results.csv: BERT Score = 0.815\n",
            "Processed gemma3_12b_results_v2.csv: BERT Score = 0.904\n",
            "Processed qwen_2.5_vl_7b_results_v2.csv: BERT Score = 0.902\n",
            "Processed smolvlm2_500m_results_v2.csv: BERT Score = 0.812\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###BFS-based Taxonomic Distance Metric"
      ],
      "metadata": {
        "id": "lU1-XKtEvrqY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# BFS-based Taxonomic Distance\n",
        "def compute_taxonomic_distance(df, taxonomy):\n",
        "    def get_distance(skill1, skill2, taxonomy):\n",
        "        if skill1 == skill2:\n",
        "            return 0\n",
        "        if skill1 not in taxonomy or skill2 not in taxonomy:\n",
        "            return 3  # Default distance for unknown skills\n",
        "        visited = set()\n",
        "        queue = [(skill1, 0)]\n",
        "        while queue:\n",
        "            skill, dist = queue.pop(0)\n",
        "            if skill == skill2:\n",
        "                return dist\n",
        "            if skill not in visited:\n",
        "                visited.add(skill)\n",
        "                for neighbor in taxonomy.get(skill, []):\n",
        "                    queue.append((neighbor, dist + 1))\n",
        "        return 3  # Max distance if no path found\n",
        "\n",
        "    ground_truth = df['ground_truth'].apply(lambda x: normalize_skill(str(x), math_synonyms))\n",
        "    predicted = df['predicted'].apply(lambda x: normalize_skill(str(x), math_synonyms))\n",
        "    distances = [get_distance(gt, pred, taxonomy) for gt, pred in zip(ground_truth, predicted)]\n",
        "    return np.mean(distances)\n",
        "\n",
        "# Evaluate BFS-based Taxonomic Distance for results\n",
        "results_taxonomic_distance = []\n",
        "for file_path in model_results:\n",
        "    try:\n",
        "        if os.path.exists(file_path):\n",
        "            df = pd.read_csv(file_path)\n",
        "            model_name = os.path.basename(file_path)\n",
        "            taxonomic_distance = compute_taxonomic_distance(df, skill_taxonomy)\n",
        "            results_taxonomic_distance.append({'Model': model_name, 'Average Taxonomic Distance': taxonomic_distance})\n",
        "            print(f\"Processed {model_name}: ATD = {taxonomic_distance:.3f}\")\n",
        "        else:\n",
        "            print(f\"File not found: {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {file_path}: {str(e)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAtUnQTxvc7y",
        "outputId": "0db97959-d586-4a28-9b05-3212d6be1086"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed internvl3_8b_results.csv: ATD = 0.930\n",
            "Processed gemma3_12b_results_v2.csv: ATD = 1.500\n",
            "Processed qwen_2.5_vl_7b_results_v2.csv: ATD = 1.490\n",
            "Processed smolvlm2_500m_results_v2.csv: ATD = 2.325\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Resnik-based Weighted Taxonomic Distance Metric"
      ],
      "metadata": {
        "id": "Zd-e7s6Dv0Rw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Resnik-based Weighted Taxonomic Distance\n",
        "def compute_weighted_taxonomic_distance(df, graph):\n",
        "    def get_distance(skill1, skill2, graph):\n",
        "        if skill1 == skill2:\n",
        "            return 0.0\n",
        "        lca = find_lca(graph, skill1, skill2)\n",
        "        if lca is None:\n",
        "            return 1.0  # Max distance if no LCA (normalized to 1)\n",
        "        ic_lca = graph.nodes[lca]['ic']\n",
        "        similarity = ic_lca\n",
        "        max_ic = max(ic.values()) if ic.values() else 1.0\n",
        "        distance = 1.0 - (similarity / max_ic) if max_ic > 0 else 1.0\n",
        "        return distance\n",
        "\n",
        "    ground_truth = df['ground_truth'].apply(lambda x: normalize_skill(str(x), math_synonyms))\n",
        "    predicted = df['predicted'].apply(lambda x: normalize_skill(str(x), math_synonyms))\n",
        "    distances = [get_distance(gt, pred, graph) for gt, pred in zip(ground_truth, predicted) if gt in graph and pred in graph]\n",
        "    return np.mean(distances) if distances else 1.0\n",
        "\n",
        "# Evaluate Resnik-based Weighted Taxonomic Distance for results\n",
        "results_weighted_taxonomic_distance = []\n",
        "for file_path in model_results:\n",
        "    try:\n",
        "        if os.path.exists(file_path):\n",
        "            df = pd.read_csv(file_path)\n",
        "            model_name = os.path.basename(file_path)\n",
        "            weighted_atd = compute_weighted_taxonomic_distance(df, G)\n",
        "            results_weighted_taxonomic_distance.append({'Model': model_name, 'Weighted Taxonomic Distance': weighted_atd})\n",
        "            print(f\"Processed {model_name}: Weighted Taxonomic Distance = {weighted_atd:.3f}\")\n",
        "        else:\n",
        "            print(f\"File not found: {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {file_path}: {str(e)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BW7wqsyfv437",
        "outputId": "2e0ef0a4-fe29-4c46-ee64-4ed3bd67020f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed internvl3_8b_results.csv: Weighted Taxonomic Distance = 0.310\n",
            "Processed gemma3_12b_results_v2.csv: Weighted Taxonomic Distance = 0.508\n",
            "Processed qwen_2.5_vl_7b_results_v2.csv: Weighted Taxonomic Distance = 0.497\n",
            "Processed smolvlm2_500m_results_v2.csv: Weighted Taxonomic Distance = 0.606\n"
          ]
        }
      ]
    }
  ]
}